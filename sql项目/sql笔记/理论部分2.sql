
理论部分

1.数据库和数据仓库区别
https://blog.csdn.net/sjmz30071360/article/details/80822085


2.BI系统结构常见
	业务数据库DB1   --|              |-- DM1
	业务数据库DB2   --|——>ODS——>DW——>|-- DM2——>RPT
	...             --|              |-- DM3
	|_________________|_____________________|_____|
			|                  |
		源系统          数据仓库架构体系    数据可视化
		
		
3.数据采集（ETL）
	https://blog.csdn.net/wl044090432/article/details/60329843
	一般采用ETL工具：Datastage、Informatica、Kettle、SSIS(SQL SERVER)、ODI(oracle)
	在技术上，ETL主要涉及到关联、转换、增量、调度和监控等几个方面！
	1.业务库到ODS采用工具还是存储过程？
	  同种数据库,一般采用存储过程（ETL工具占用资源比较大,消耗内存影响服务器）,不同数据库,采用ETL工具
	2.采集数据注意问题？
	  <1>抽取方式：
	     全量抽取
	     增量抽取：时间戳/触发器/全表比对/日志对比
	  <2>抽取频率：按天/按周/按月
	  <3>数据转换：数据格式的不一致、数据输入错误、数据不完整等等
	  <4>ETL工具的选择:
		在数据集成中该如何选择ETL工具呢？一般来说需要考虑以下几个方面：
		(1)对平台的支持程度。
		(2)对数据源的支持程度。
		(3)抽取和装载的性能是不是较高，且对业务系统的性能影响大不大，倾入性高不高。
		(4)数据转换和加工的功能强不强。
		(5)是否具有管理和调度功能。
		(6)是否具有良好的集成性和开放性。
	  <5>调度：shell/python脚本 + crontab, control-M,azkaban等
	  <6>调度时间：凌晨之后
	  
	  
4.建模
   https://segmentfault.com/a/1190000012882641
   1.范式建模（三范式）         -- 数仓比较常用  （不会造成数据的冗余）
   2.维度建模（维度表和事实表） -- 集市比较常用  （为了多维度的统计数据，一般做成大宽表）
   3.实体建模
   
   
   宽表和窄表、明细表和汇总表、事实表和维度表等
   https://www.cnblogs.com/Leo_wl/p/8515794.html
   https://www.cnblogs.com/xyg-zyx/p/9803580.html
   
   建模工具：ER-WIN、PowerDesigner
   通过对业务熟悉,采取相关的模型概念,设计表结构以及表之间的关联关系。
   最后导出建表语句到相应的数据库中生成表结构。
   
   
5.数据可视化（表格和图形）
  报表工具：SAP BO、Cognos、 Oracle BIEE、Qlikview、Tableau、Powerbi、Smartbi、FineBI、Finereport
  设计报表的表结构,编写存储过程或SQL加工报表数据,用工具制作报表。

   
   
6.跨库查询数据和编写存储过程
	database links（不同库） 区分同义词synonyms（同库不同用户）
	-- 创建
	CREATE PUBLIC DATABASE LINK DB_ORCL  
	CONNECT TO SCOTT IDENTIFIED BY "123456"  -- 数字密码"123456"  
	USING '192.168.3.3:1521/orcl';         -- 需要连接的数据IP/端口/数据库名



7.查看执行计划（主要是sql调优)
  （1）可以查看sql语句的执行顺序
	  1.箭头下一步可以查看执行先后顺序
	  2.看缩进：
		（1）缩进最多的最先执行（缩进相同时,最上面的最先执行,接着执行其父节点）
		（2）执行父节点的并列节点的子节点（多个并列节点,从上到下,接着执行其父节点）
		（3）执行父节点的上一个节点
		（4）依次类推,执行完所有节点为止
  （2）可以查看表的扫描方式（全表扫描还/索引扫描）
  （3）可以查看表的关联机制（SORT MERGE JOIN/HASH JOIN/NESTED LOOPS JOIN）
  
  
  
8.关联机制
	https://www.cnblogs.com/polestar/p/4132911.html
	在oracle中,关联机制有三种方式：嵌套循环、哈希连接、（归并）排序合并连接

   
9.HINTS
	https://www.cnblogs.com/eastsea/p/3799423.html
	主要：1.强制走索引 2.改变关联机制 3.程序并发数 4.提高插入数据速度（append)
	--常用HINTS
	1、/*+ INDEX(表名,索引名) */        --指定索引
	2、/*+ USE_MERGE(表名1,表名2) */    --指定用排序合并连接（USE_HASH或者USE_NL）
	3、/*+ PARALLEL(表名1,并行数)[(表名2,并行数)……] */ --指定开启多少个并行|并发（一般为2、4、8……）
	4、/*+ APPEND */                    --数据直接插入到高水位上面(与insert连用)直接往后面插,无视前面的空位置,可以提高插入数据速度
   
   
10.HWM高水位（产生原因和解决方法）
   https://blog.csdn.net/weixin_33807284/article/details/91969340
   
   
11.优化问题
	1.硬件优化       -- 增加服务器运行内存,增大磁盘空间
	2.软件方面优化
	  （1）分表
	  （2）分区
	  （3）索引（分区索引）
	  （4）执行计划
	  （5）HINTS 强制优化
	  （6）其它：建表,书写sql都常规注意事项（varchar2替换char,null非特殊情况用其它值替换,小表放后,where 过滤条件大的放在最后等等）
	  
	  
		注意：1.最好对关联字段建立索引
			  2.oracle中,from中和where中都是从后往前执行。
			  （1）from a,b ,则b应该为小表
			  （2）where  条件1 and 条件2 则条件2为能过滤掉大部分数据（或挑选出比较少数据）的条件

        优化：存储过程比较耗时,怎么解决问题？
		    （1）从日志记录表查看耗时比较长的存储过程
			（2）在存储过程中查看每一段命令的耗时,将耗时比较长的语句找出来
			（3）语句中先看各个单表查询耗时(表被锁或被占用)
			（4）关联字段是否建立索引
			（5）索引是否失效（排除常规语句导致索引失效,去查看执行计划）
			（6）执行计划查看关联机制
			（7）调整关联机制（强制改变）
            （8）表太大,如果没有分区考虑建立分区
	  
	  
11.缓慢变化维
	https://www.cnblogs.com/xqzt/p/4472005.html
	在从 OLTP 业务数据库向 OLAP 数据仓库抽取数据的过程中,
	特别是第一次导入之后的每一次增量抽取往往会遇到这样的问题：
	业务数据库中的一些数据发生了更改,到底要不要将这些变化也反映到数据仓库中？
	在数据仓库中,哪些数据应该随之变化,哪些可以不用变化？
	考虑到这些变化,在数据仓库中的维度表又应该如何设计以满足这些需要。
	Type 1 SCD :不记录历史数据,新数据覆盖旧数据
	Type 2 SCD: 保存多条记录,直接新添一条记录,同时保留原有记录,并用单独的专用的字段保存区别
	Type 3 SCD：添加历史列,用不同的字段保存变化痕迹.它只能保存两次变化记录.适用于变化不超过两次的维度。
	
	
	拉链表：一般是缓慢变化维的Type 2的实现方式！
	https://www.cnblogs.com/lxbmaomao/p/9821128.html
	  
	  
12.物化视图（普通视图和物化视图的区别）	 
   https://blog.csdn.net/joshua_peng1985/article/details/6213593
  （1）物理结构,占用空间
  （2）可以建立索引
  （3）可以按照需求根据基础表刷新数据
	  
	  
13.关联方式实战（不考虑where过滤）：
	A表 15条数据
	B表 4条数据
	1.内关联 有多少条数据？      0~60
    2.A左外关联B  有多少条数据？ 15~60
    3.A全外关联B  有多少条数据？ 15~60
	
14.递归查询
   https://www.cnblogs.com/wanghonghu/archive/2012/08/31/2665945.html


15.正则表达式：
   https://www.cnblogs.com/Marydon20170307/p/7884726.html


16.显示游标在工作中应用的情况？
   (1)遍历游标,效率不高
      <1>数据特别大,需要分批提交数据
	  <2>逐行进行条件判断后处理
   (2)临时表替换
      <1>数据不是很大
	  <2>不需要对查询结果集进行多条件判断处理
	  

17.批量加载 forall
   https://www.cnblogs.com/shcqupc/p/5105817.html
	   

18.数据块 BULK
   https://www.cnblogs.com/xwb583312435/p/9055182.html
	  
	  
	  
	  

	  
	  
	  
	
	  
	  
	
	
	
	