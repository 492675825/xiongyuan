{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitbasecondad9bd0a7157c84fa7bace955ad09d0846",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Word2Vec文本处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec  #自然语言处理\n",
    "import logging  #格式美化\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import jieba\n",
    "import re, string\n",
    "from zhon.hanzi import punctuation #去除中文中的所有标点符号\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s:%(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造文本\n",
    "raw_sentences = ['the quick brown fox jumps over the lazy dogs','yoyoyo you go home now to sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "#拆分单词\n",
    "sentences = [s.split() for s in raw_sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-25 00:40:30,903 : INFO:collecting all words and their counts\n",
      "2021-01-25 00:40:30,906 : INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-25 00:40:30,908 : INFO:collected 15 word types from a corpus of 16 raw words and 2 sentences\n",
      "2021-01-25 00:40:30,911 : INFO:Loading a fresh vocabulary\n",
      "2021-01-25 00:40:30,914 : INFO:effective_min_count=1 retains 15 unique words (100% of original 15, drops 0)\n",
      "2021-01-25 00:40:30,919 : INFO:effective_min_count=1 leaves 16 word corpus (100% of original 16, drops 0)\n",
      "2021-01-25 00:40:30,922 : INFO:deleting the raw counts dictionary of 15 items\n",
      "2021-01-25 00:40:30,926 : INFO:sample=0.001 downsamples 15 most-common words\n",
      "2021-01-25 00:40:30,928 : INFO:downsampling leaves estimated 2 word corpus (13.7% of prior 16)\n",
      "2021-01-25 00:40:30,930 : INFO:estimated required memory for 15 words and 100 dimensions: 19500 bytes\n",
      "2021-01-25 00:40:30,931 : INFO:resetting layer weights\n",
      "2021-01-25 00:40:30,944 : INFO:training model with 3 workers on 15 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-25 00:40:30,948 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:40:30,950 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:40:30,951 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:40:30,952 : INFO:EPOCH - 1 : training on 16 raw words (2 effective words) took 0.0s, 454 effective words/s\n",
      "2021-01-25 00:40:30,962 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:40:30,964 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:40:30,965 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:40:30,966 : INFO:EPOCH - 2 : training on 16 raw words (3 effective words) took 0.0s, 618 effective words/s\n",
      "2021-01-25 00:40:30,973 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:40:30,977 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:40:30,979 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:40:30,980 : INFO:EPOCH - 3 : training on 16 raw words (1 effective words) took 0.0s, 127 effective words/s\n",
      "2021-01-25 00:40:30,986 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:40:30,987 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:40:30,989 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:40:30,990 : INFO:EPOCH - 4 : training on 16 raw words (2 effective words) took 0.0s, 386 effective words/s\n",
      "2021-01-25 00:40:30,997 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:40:30,999 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:40:31,001 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:40:31,002 : INFO:EPOCH - 5 : training on 16 raw words (2 effective words) took 0.0s, 359 effective words/s\n",
      "2021-01-25 00:40:31,004 : INFO:training on a 80 raw words (10 effective words) took 0.1s, 169 effective words/s\n",
      "2021-01-25 00:40:31,005 : WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "#实例化word2vec模块\n",
    "'''\n",
    "    min_count=1:过滤掉只出现过1次的词\n",
    "    size:神经网络的层数，默认是100\n",
    "    workers:用于设置并发训练时候的线程数，仅当Cython安装的情况下才会起作用\n",
    "'''\n",
    "model = word2vec.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.07596941"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "#查看单词之间的相似度\n",
    "model.similarity('dogs','you')"
   ]
  },
  {
   "source": [
    "维基百科数据下载\n",
    "https://dumps.wikimedia.org/zhwiki/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nt2s - 繁体转简体\\ns2t - 简体转繁体\\nmix2t - 混合体转繁体\\nmix2s - 混合体转简体\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "#是用opencc-python来转换繁体字\n",
    "'''\n",
    "t2s - 繁体转简体\n",
    "s2t - 简体转繁体\n",
    "mix2t - 混合体转繁体\n",
    "mix2s - 混合体转简体\n",
    "'''\n",
    "#import opencc\n",
    "#cc = opencc.OpenCC('t2s')\n",
    "#print cc.convert(u'Open Chinese Convert（OpenCC）「開放中文轉換」')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'./平凡的世界.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造分词的方法\n",
    "f=open('./平凡的世界.txt',encoding='utf8')\n",
    "data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清除所有的标点符号\n",
    "data_clean = re.sub(r'[%s]+' %punctuation, '',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉所有的空格\n",
    "data_clean_ = data_clean.replace(\" \",\"\")\n",
    "data_clean_ = data_clean_.replace(\"\\n\",\"\")\n",
    "data_clean_ = data_clean_.replace(\"---\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#结巴分词\n",
    "text = jieba.cut(data_clean_, cut_all=False)\n",
    "new_test = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1140534"
      ]
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "#保存分好的词\n",
    "testdate=open('./test.txt','w',encoding='utf-8')\n",
    "testdate.write(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用lineSentence读取出分好的词，才能顺利构建模型\n",
    "sentences_ = word2vec.LineSentence('./test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-25 00:53:25,147 : INFO:collecting all words and their counts\n",
      "2021-01-25 00:53:25,242 : INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-25 00:53:25,432 : INFO:collected 35431 word types from a corpus of 428890 raw words and 43 sentences\n",
      "2021-01-25 00:53:25,433 : INFO:Loading a fresh vocabulary\n",
      "2021-01-25 00:53:25,536 : INFO:effective_min_count=1 retains 35431 unique words (100% of original 35431, drops 0)\n",
      "2021-01-25 00:53:25,537 : INFO:effective_min_count=1 leaves 428890 word corpus (100% of original 428890, drops 0)\n",
      "2021-01-25 00:53:25,734 : INFO:deleting the raw counts dictionary of 35431 items\n",
      "2021-01-25 00:53:25,736 : INFO:sample=0.001 downsamples 39 most-common words\n",
      "2021-01-25 00:53:25,739 : INFO:downsampling leaves estimated 352335 word corpus (82.2% of prior 428890)\n",
      "2021-01-25 00:53:25,890 : INFO:estimated required memory for 35431 words and 100 dimensions: 46060300 bytes\n",
      "2021-01-25 00:53:25,891 : INFO:resetting layer weights\n",
      "2021-01-25 00:53:36,861 : INFO:training model with 3 workers on 35431 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-01-25 00:53:37,594 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:53:37,630 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:53:37,633 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:53:37,633 : INFO:EPOCH - 1 : training on 428890 raw words (352208 effective words) took 0.8s, 457861 effective words/s\n",
      "2021-01-25 00:53:38,474 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:53:38,504 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:53:38,507 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:53:38,509 : INFO:EPOCH - 2 : training on 428890 raw words (352373 effective words) took 0.9s, 404548 effective words/s\n",
      "2021-01-25 00:53:39,324 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:53:39,342 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:53:39,364 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:53:39,366 : INFO:EPOCH - 3 : training on 428890 raw words (352288 effective words) took 0.9s, 413391 effective words/s\n",
      "2021-01-25 00:53:40,319 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:53:40,322 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:53:40,336 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:53:40,339 : INFO:EPOCH - 4 : training on 428890 raw words (352340 effective words) took 1.0s, 364243 effective words/s\n",
      "2021-01-25 00:53:41,095 : INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-25 00:53:41,111 : INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-25 00:53:41,116 : INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-25 00:53:41,118 : INFO:EPOCH - 5 : training on 428890 raw words (352354 effective words) took 0.8s, 457443 effective words/s\n",
      "2021-01-25 00:53:41,119 : INFO:training on a 2144450 raw words (1761563 effective words) took 4.3s, 413728 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#建立word2vec模型\n",
    "model_ = word2vec.Word2Vec(sentences_, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "美好\n[('生命', 0.9991242289543152), ('精神', 0.9986003041267395), ('艰难', 0.9983970522880554), ('现实', 0.998322069644928), ('复杂', 0.9980694055557251), ('情感', 0.9979764819145203), ('欢乐', 0.9977883100509644), ('社会', 0.9974801540374756), ('矛盾', 0.9974163770675659), ('所', 0.9973421096801758)]\n平凡\n[('所有', 0.9970272779464722), ('经历', 0.9962289333343506), ('感情', 0.9948828816413879), ('生命', 0.9933726191520691), ('现实', 0.9933608770370483), ('梦幻', 0.9932478070259094), ('却是', 0.9929790496826172), ('普通人', 0.9923515319824219), ('男生', 0.9923454523086548), ('每', 0.9919865131378174)]\n女人\n[('这是', 0.9984006881713867), ('还有', 0.9983314275741577), ('教师', 0.997976541519165), ('维持', 0.9978894591331482), ('高朗', 0.9978026151657104), ('满足', 0.997765302658081), ('命运', 0.997738242149353), ('人来', 0.9976729154586792), ('活人', 0.9974938631057739), ('东西', 0.997492790222168)]\n"
     ]
    }
   ],
   "source": [
    "#用训练好的模型进行预测\n",
    "testwords = ['美好','平凡','女人']\n",
    "for words in testwords:\n",
    "    res = model_.most_similar(words)\n",
    "    print(words)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}